{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nAuto-scheduling a convolution layer for GPU\n===========================================\n**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             `Chengfan Jia <https://github.com/jcf94/>`_\n\n\nDifferent from the existing `autotvm <tutorials-autotvm-sec>` which relies on \nmanual templates to define the search space, the auto-scheduler does not require any templates.\nThe auto-scheduler is template-free, so users only need to write the computation declaration without\nany schedule commands or templates.\nThe auto-scheduler can automatically generate a large\nsearch space and find a good schedule in the space.\n\nWe use a convolution layer as an example in this tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tvm\nfrom tvm import te, testing, auto_scheduler, topi\nfrom tvm.topi.testing import conv2d_nchw_python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the computation\n^^^^^^^^^^^^^^^^^^^^^^\nTo begin with, let us define the computation of a convolution layer.\nThe function should return the list of input/output tensors.\nFrom these tensors, the auto-scheduler can get the whole computational graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@auto_scheduler.register_workload\ndef conv2d_layer(N, H, W, CO, CI, KH, KW, stride, padding):\n    data = te.placeholder((N, CI, H, W), name=\"data\")\n    kernel = te.placeholder((CO, CI, KH, KW), name=\"kernel\")\n    bias = te.placeholder((1, CO, 1, 1), name=\"bias\")\n    conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype=\"float32\")\n    out = topi.nn.relu(conv + bias)\n    return [data, kernel, bias, out]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the search task\n^^^^^^^^^^^^^^^^^^^^^^\nWe then create a search task for the last convolution layer in the resnet.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = tvm.target.Target(\"cuda\")\n\n# the last layer in resnet\nN, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\ntask = auto_scheduler.create_task(conv2d_layer, (N, H, W, CO, CI, KH, KW, strides, padding), target)\n\n# Inspect the computational graph\nprint(task.compute_dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set parameters for the auto-scheduler. These parameters\nmainly specify how we do the measurement during the search and auto-tuning.\n\n* `measure_ctx` launches a different process for measurement. This\n  provides an isolation. It can protect the master process from GPU crashes\n  happended during measurement and avoid other runtime conflicts.\n* `min_repeat_ms` defines the minimum duration of one \"repeat\" in every measurement.\n  This can warmup the GPU, which is necessary to get accurate measurement results.\n  Typically, we recommend a value > 300 ms.\n* `num_measure_trials` is the number of measurement trials we can use during the search.\n  We only make 10 trials in this tutorial for a fast demonstration. In practice, 1000 is a\n  good value for the search to converge. You can do more trials according to your time budget.\n* In addition, we use `RecordToFile` to dump measurement records into a file `conv2d.json`.\n  The measurement records can be used to query the history best, resume the search,\n  and do more analyses later.\n* see :any:`auto_scheduler.auto_schedule.TuningOptions`:,\n  :any:`auto_scheduler.measure.LocalRPCMeasureContext` for more parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)\ntune_option = auto_scheduler.TuningOptions(\n    num_measure_trials=10,\n    runner=measure_ctx.runner,\n    measure_callbacks=[auto_scheduler.RecordToFile(\"conv2d.json\")],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the search\n^^^^^^^^^^^^^^\nNow we get all inputs ready. Pretty simple, isn't it?\nWe can kick off the search and let the auto-scheduler do its magic.\nAfter some measurement trials, it will return the best schedule it found.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch, args = auto_scheduler.auto_schedule(task, tuning_options=tune_option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can lower the schedule to see the IR after auto-scheduling.\nThe auto-scheduler correctly performs optimizations including multi-level tiling,\ncooperative fetching, unrolling and operator fusion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tvm.lower(sch, args, simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check correctness and evaluate performance\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWe build the binary and check its correctness and performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "func = tvm.build(sch, args, target)\n\n# check correctness\ndata_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)\nweight_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)\nbias_np = np.random.uniform(size=(1, CO, 1, 1)).astype(np.float32)\nconv_np = conv2d_nchw_python(data_np, weight_np, strides, padding)\nout_np = np.maximum(conv_np + bias_np, 0.0)\n\nctx = tvm.gpu()\ndata_tvm = tvm.nd.array(data_np, ctx=ctx)\nweight_tvm = tvm.nd.array(weight_np, ctx=ctx)\nbias_tvm = tvm.nd.array(bias_np, ctx=ctx)\nout_tvm = tvm.nd.empty(out_np.shape, ctx=ctx)\nfunc(data_tvm, weight_tvm, bias_tvm, out_tvm)\n\n# Check results\ntvm.testing.assert_allclose(out_np, out_tvm.asnumpy(), rtol=1e-3)\n\n# Evaluate execution time\nevaluator = func.time_evaluator(func.entry_name, ctx, min_repeat_ms=500)\nprint(\n    \"Execution time of this operator: %.3f ms\"\n    % (np.median(evaluator(data_tvm, weight_tvm, bias_tvm, out_tvm).results) * 1000)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the record file\n^^^^^^^^^^^^^^^^^^^^^\nDuring the search, all measuremnt records are dumpped into the record\nfile \"conv2d.json\". The measurement records can be used to re-apply search results,\nresume the search, and perform other analyses.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is an example where we load the best schedule from a file,\nprint the equivalent python schedule API, and build the binary again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the measuremnt record for the best schedule\ninp, res = auto_scheduler.load_best(\"conv2d.json\", task.workload_key)\n\n# Print equivalent python schedule API. This can be used for debugging and\n# learning the behavior of the auto-scheduler.\nprint(\"Equivalent python schedule:\")\nprint(task.compute_dag.print_python_code_from_state(inp.state))\n\n# Rebuild the binary. This shows how you can apply the best schedule from a\n# log file without reruning the search again.\nsch, args = task.compute_dag.apply_steps_from_state(inp.state)\nfunc = tvm.build(sch, args, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more complicated example is to resume the search.\nIn this case, we need to create the search policy and cost model by ourselves\nand resume the status of search policy and cost model with the log file.\nIn the example below we resume the status and do more 5 trials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_file = \"conv2d.json\"\ncost_model = auto_scheduler.XGBModel()\ncost_model.update_from_file(log_file)\nsearch_policy = auto_scheduler.SketchPolicy(\n    task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]\n)\ntune_option = auto_scheduler.TuningOptions(\n    num_measure_trials=5,\n    runner=measure_ctx.runner,\n    measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n)\nsch, args = auto_scheduler.auto_schedule(task, search_policy, tuning_options=tune_option)\n\n# kill the measurement process\ndel measure_ctx"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}